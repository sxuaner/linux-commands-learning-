# what is wget 
GNU Wget is a free utility for non-interactive download of files from the Web.  It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies.

Wget is non-interactive, meaning that it can work in the background, while the user is not logged on.  This allows you to start a retrieval and disconnect from the system,
letting Wget finish the work.  By contrast, most of the Web browsers require constant user's presence, which can be a great hindrance when transferring a lot of data.

Wget can follow links in HTML, XHTML, and CSS pages, to create local versions of remote web sites, fully recreating the directory structure of the original site.  This is
sometimes referred to as "recursive downloading."  While doing that, Wget respects the Robot Exclusion Standard (/robots.txt).  Wget can be instructed to convert the links in
downloaded files to point at the local files, for offline viewing.

Wget has been designed for robustness over slow or unstable network connections; if a download fails due to a network problem, it will keep retrying until the whole file has
been retrieved.  If the server supports regetting, it will instruct the server to continue the download from where it left off.


# To download a page and all links inside
Recursive Retrieval Options
-r
--recursive
    Turn on recursive retrieving.    The default maximum depth is 5.

-l depth
--level=depth
    Set the maximum number of subdirectories that Wget will recurse into to depth.  In order to prevent one from accidentally downloading very large websites when using
    recursion this is limited to a depth of 5 by default, i.e., it will traverse at most 5 directories deep starting from the provided URL.  Set -l 0 or -l inf for infinite
    recursion depth.

            wget -r -l 0 http://<site>/1.html

    Ideally, one would expect this to download just 1.html.  but unfortunately this is not the case, because -l 0 is equivalent to -l inf---that is, infinite recursion.  To
    download a single HTML page (or a handful of them), specify them all on the command line and leave away -r and -l. To download the essential items to view a single HTML
    page, see page requisites.
